import os
import re
import json
import logging
from typing import List, Tuple, Optional

import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel

# LangChain subpackages
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# --- æ—¥å¿—é…ç½® ---
# (åœ¨ Streamlit ä¸­ï¼Œæ—¥å¿—ä¸»è¦è¾“å‡ºåˆ°ç»ˆç«¯)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("legal_rag_app")

# -------------------- config (è¯·æ ¸å¯¹è¿™äº›è·¯å¾„) --------------------
# åµŒå…¥æ¨¡å‹ (å¿…é¡»ä¸ build_index.py ä¸­çš„ä¸€è‡´)
EMB_MODEL_PATH = "../bge-large-zh-v1.5/"
# Base LLM æ¨¡å‹
BASE_MODEL_PATH = "../Baichuan2-7B-Base/"
# LoRA adapter è·¯å¾„ (è‹¥æ— è¯·è®¾ä¸º None)
LORA_ADAPTER_PATH = "../lora_new_sft_adapter/"
# ç´¢å¼•åŠ è½½è·¯å¾„ (å¿…é¡»æ˜¯ build_index.py ç”Ÿæˆçš„)
FAISS_INDEX_DIR = "faiss_legal_qa_index"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOP_K = 5
# -----------------------------------------------------------------


# === ä» rag_manual_v1.py å¤åˆ¶çš„æ ¸å¿ƒå‡½æ•° ===

def init_generation_model(base_model_path: str, lora_adapter_path: Optional[str] = None):
    """
    åŠ è½½ LLM å’Œ Tokenizerï¼Œå¹¶åŒ…è£…æˆ pipelineã€‚
    ä¸ä½ çš„ rag_manual_v1.py ä¸­çš„å‡½æ•°ç›¸åŒã€‚
    """
    # åŠ è½½åŸºç¡€æ¨¡å‹
    dtype = torch.float16 if DEVICE.startswith("cuda") else torch.float32
    logger.info("åŠ è½½ base æ¨¡å‹ (dtype=%s, device=%s)...", dtype, DEVICE)
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_model_path):
        logger.error("!!! è‡´å‘½é”™è¯¯: æ‰¾ä¸åˆ° Base æ¨¡å‹è·¯å¾„: %s", base_model_path)
        st.error(f"é”™è¯¯: æ‰¾ä¸åˆ° Base æ¨¡å‹è·¯å¾„: {base_model_path}")
        st.stop()
        
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path, torch_dtype=dtype, device_map="auto", trust_remote_code=True
    )
    
    # å¦‚æœ‰ LoRA adapterï¼ŒåŠ è½½
    if lora_adapter_path and os.path.exists(lora_adapter_path):
        try:
            model = PeftModel.from_pretrained(base_model, lora_adapter_path)
            logger.info("LoRA adapter ä» %s åŠ è½½æˆåŠŸ", lora_adapter_path)
        except Exception as e:
            logger.warning("åŠ è½½ LoRA å¤±è´¥ (%s)ï¼Œå°†ä½¿ç”¨ base modelã€‚", e)
            model = base_model
    elif lora_adapter_path:
        logger.warning("æä¾›äº† LoRA è·¯å¾„ä½†æœªæ‰¾åˆ°: %sï¼Œå°†ä½¿ç”¨ base modelã€‚", lora_adapter_path)
        model = base_model
    else:
        logger.info("æœªæä¾› LoRA è·¯å¾„ï¼Œä½¿ç”¨ base modelã€‚")
        model = base_model
        
    model.use_cache = False
    
    # tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True, use_fast=False)
    
    # wrap pipeline
    gen_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device_map="auto",
        torch_dtype=dtype,
        max_new_tokens=512,  # é»˜è®¤å€¼
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        pad_token_id=getattr(tokenizer, "eos_token_id", tokenizer.pad_token_id),
    )
    logger.info("æ–‡æœ¬ç”Ÿæˆ pipeline åˆ›å»ºå®Œæˆ")
    return model, tokenizer, gen_pipe


def compose_prompt_with_context(question: str, docs: List[Document]) -> str:
    """
    ç®€å• prompt æ‹¼æ¥ç­–ç•¥ã€‚
    ä¸ä½ çš„ rag_manual_v1.py ä¸­çš„å‡½æ•°ç›¸åŒã€‚
    """
    ctxs = []
    for i, d in enumerate(docs, 1):
        snippet = d.page_content.strip()
        ctxs.append(f"[{i}] {snippet}")
    context_block = "\n\n".join(ctxs)
    
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå…·æœ‰æ³•å¾‹ä¸“ä¸šçŸ¥è¯†çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä»…åŸºäºä¸‹é¢æä¾›çš„ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œ"
        "å¹¶åœ¨ç­”æ¡ˆæœ«å°¾åˆ—å‡ºä½ å¼•ç”¨çš„æ–‡æ¡£ç¼–å·ã€‚\n\n"
        f"Context:\n{context_block}\n\nQuestion: {question}\n\nAnswer:"
    )
    return prompt


def answer_by_rag(gen_pipe, vector_db: FAISS, question: str, k: int = TOP_K) -> dict:
    """
    æ‰§è¡Œ RAG æµç¨‹ï¼ˆæ£€ç´¢ã€æ„å»ºPromptã€ç”Ÿæˆï¼‰ã€‚
    ä¸ä½ çš„ rag_manual_v1.py ä¸­çš„å‡½æ•°ç›¸åŒã€‚
    """
    # 1. æ£€ç´¢
    logger.info("æ­£åœ¨ä¸ºé—®é¢˜æ£€ç´¢: %s", question[:50] + "...")
    try:
        retrieved = vector_db.similarity_search(question, k=k)
        logger.info("æ£€ç´¢åˆ° %d æ¡æ–‡æ¡£", len(retrieved))
    except Exception as e:
        logger.error("FAISS æ£€ç´¢å¤±è´¥: %s", e)
        return {"answer": f"æŠ±æ­‰ï¼Œæ£€ç´¢æ–‡æ¡£æ—¶å‡ºé”™: {e}", "source_documents": []}

    # 2. æ„å»º prompt
    prompt = compose_prompt_with_context(question, retrieved)

    # 3. ç”Ÿæˆ
    logger.info("å¼€å§‹ç”Ÿæˆç­”æ¡ˆ...")
    try:
        out = gen_pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9)
        raw_text = out[0].get("generated_text", "")
        
        # å»æ‰ prompt éƒ¨åˆ†
        if raw_text.startswith(prompt):
            answer = raw_text[len(prompt):].strip()
        else:
            answer = raw_text.strip()
            
        logger.info("ç­”æ¡ˆç”Ÿæˆå®Œæ¯•ã€‚")
    except Exception as e:
        logger.error("æ¨¡å‹ç”Ÿæˆå¤±è´¥: %s", e)
        answer = f"æŠ±æ­‰ï¼Œæ¨¡å‹ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"

    # 4. è¿”å› answer + sources
    result = {
        "answer": answer,
        "source_documents": retrieved # è¿”å›åŸå§‹ Document å¯¹è±¡
    }
    return result

# === Streamlit ç¼“å­˜åŠ è½½å‡½æ•° ===

@st.cache_resource
def load_faiss_index(_emb_model_path, _index_dir):
    """
    (ä»… Streamlit) ç¼“å­˜åŠ è½½ FAISS ç´¢å¼•å’ŒåµŒå…¥æ¨¡å‹ã€‚
    """
    logger.info("--- æ­£åœ¨åŠ è½½ FAISS ç´¢å¼•å’Œ BGE åµŒå…¥æ¨¡å‹ ---")
    if not os.path.exists(_emb_model_path):
        logger.error("!!! è‡´å‘½é”™è¯¯: æ‰¾ä¸åˆ°åµŒå…¥æ¨¡å‹è·¯å¾„: %s", _emb_model_path)
        st.error(f"é”™è¯¯: æ‰¾ä¸åˆ°åµŒå…¥æ¨¡å‹è·¯å¾„: {_emb_model_path}")
        st.stop()
    if not os.path.exists(_index_dir):
        logger.error("!!! è‡´å‘½é”™è¯¯: æ‰¾ä¸åˆ° FAISS ç´¢å¼•: %s", _index_dir)
        st.error(f"é”™è¯¯: æ‰¾ä¸åˆ° FAISS ç´¢å¼•ç›®å½•: {_index_dir}")
        st.info("è¯·å…ˆè¿è¡Œ `build_index.py` è„šæœ¬æ¥åˆ›å»ºç´¢å¼•ã€‚")
        st.stop()
        
    try:
        emb = HuggingFaceEmbeddings(
            model_name=_emb_model_path,
            model_kwargs={"device": DEVICE},
            encode_kwargs={"normalize_embeddings": True, "batch_size": 32},
        )
        vector_db = FAISS.load_local(_index_dir, emb, allow_dangerous_deserialization=True)
        logger.info("--- FAISS ç´¢å¼•å’Œ BGE åŠ è½½å®Œæˆ ---")
        return vector_db
    except Exception as e:
        logger.error("!!! è‡´å‘½é”™è¯¯: åŠ è½½ FAISS ç´¢å¼•å¤±è´¥: %s", e)
        st.error(f"åŠ è½½ FAISS ç´¢å¼•å¤±è´¥: {e}")
        st.stop()


@st.cache_resource
def load_llm_pipeline(_base_model_path, _lora_adapter_path):
    """
    (ä»… Streamlit) ç¼“å­˜åŠ è½½ LLM ç”Ÿæˆ pipelineã€‚
    """
    logger.info("--- æ­£åœ¨åŠ è½½ LLM (Baichuan2 + LoRA) ---")
    # å¤ç”¨ä½ çš„å‡½æ•°
    _, _, gen_pipe = init_generation_model(_base_model_path, _lora_adapter_path)
    logger.info("--- LLM Pipeline åŠ è½½å®Œæˆ ---")
    return gen_pipe

# === Streamlit UI ç•Œé¢ ===

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="æ³•å¾‹å¤§æ¨¡å‹ RAG é—®ç­”",
    page_icon="âš–ï¸",
    initial_sidebar_state="collapsed"
)

# --- CSS æ ·å¼ (æ¥è‡ªä½ çš„ minimind ç¤ºä¾‹) ---
st.markdown("""
    <style>
        /* (è¿™é‡Œçœç•¥äº†ä½ æä¾›çš„é•¿ä¸² CSSï¼Œä¿æŒåŸæ ·) */
        /* ... ä½ æä¾›çš„æ‰€æœ‰ .stButton, .stMainBlockContainer, .stApp æ ·å¼ ... */
        
        /* èŠå¤©æ¶ˆæ¯æ ·å¼ */
        .stChatMessage {
            border-radius: 10px;
            padding: 12px;
            margin-bottom: 10px;
        }
        /* ç”¨æˆ·æ¶ˆæ¯ (é å³) */
        div[data-testid="chat-message-container"]:has(div[data-testid="stChatMessageContent"][style*="flex-end"]) {
            /* ä¹Ÿè®¸æ·»åŠ ä¸€ä¸ªèƒŒæ™¯è‰² */
        }
        /* åŠ©æ‰‹æ¶ˆæ¯ (é å·¦) */
        div[data-testid="chat-message-container"]:has(div[data-testid="stChatMessageContent"][style*="flex-start"]) {
            /* ä¹Ÿè®¸æ·»åŠ ä¸€ä¸ªèƒŒæ™¯è‰² */
        }
        
        /* æ¥æºæ–‡æ¡£çš„æ ·å¼ */
        .source-container {
            border-top: 1px solid #eee;
            margin-top: 15px;
            padding-top: 10px;
        }
        .source-item {
            font-size: 0.9em;
            color: #555;
            background-color: #f9f9f9;
            border-radius: 5px;
            padding: 8px;
            margin-bottom: 5px;
            border: 1px solid #eee;
        }
        .source-item summary {
            font-weight: bold;
            cursor: pointer;
        }
    </style>
""", unsafe_allow_html=True)

# --- æ ‡é¢˜å’ŒSlogan ---
st.markdown(
    f'<div style="display: flex; flex-direction: column; align-items: center; text-align: center; margin: 0; padding: 0;">'
    '<div style="font-style: italic; font-weight: 900; margin: 0; padding-top: 4px; display: flex; align-items: center; justify-content: center; flex-wrap: wrap; width: 100%;">'
    f'<span style="font-size: 40px; margin-right: 10px;">âš–ï¸</span>'
    f'<span style="font-size: 26px; margin-left: 10px;">æ³•å¾‹å¤§æ¨¡å‹ RAG é—®ç­”</span>'
    '</div>'
    '<span style="color: #bbb; font-style: italic; margin-top: 6px; margin-bottom: 10px;">å†…å®¹ç”±AIç”Ÿæˆï¼Œå¹¶åŸºäºæ£€ç´¢çš„æ³•æ¡ï¼Œè¯·ä»”ç»†ç”„åˆ«</span>'
    '</div>',
    unsafe_allow_html=True
)

# --- åŠ è½½æ¨¡å‹å’Œç´¢å¼• ---
# (è¿™ä¼šåœ¨é¡µé¢é¦–æ¬¡åŠ è½½æ—¶è¿è¡Œï¼Œå¹¶ç¼“å­˜ç»“æœ)
try:
    vector_db = load_faiss_index(EMB_MODEL_PATH, FAISS_INDEX_DIR)
    gen_pipe = load_llm_pipeline(BASE_MODEL_PATH, LORA_ADAPTER_PATH)
    st.success("æ¨¡å‹å’Œç´¢å¼•åŠ è½½æˆåŠŸï¼", icon="âœ…")
except Exception as e:
    # é”™è¯¯å·²åœ¨åŠ è½½å‡½æ•°å†…éƒ¨å¤„ç† (st.error + st.stop)
    pass


# --- èŠå¤©ç•Œé¢ ---

# åˆå§‹åŒ– session_state
if "messages" not in st.session_state:
    st.session_state.messages = []

# åœ¨ä¾§è¾¹æ æ·»åŠ æ¸…ç©ºæŒ‰é’®
st.sidebar.title("é€‰é¡¹")
if st.sidebar.button("æ¸…ç©ºèŠå¤©è®°å½•", use_container_width=True):
    st.session_state.messages = []
    st.rerun() # é‡æ–°è¿è¡Œä»¥æ¸…ç©ºç•Œé¢

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    avatar = "âš–ï¸" if message["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(message["role"], avatar=avatar):
        # åŠ©æ‰‹æ¶ˆæ¯å¯èƒ½åŒ…å« HTML (ç”¨äºæ¥æº)
        if message["role"] == "assistant":
            st.markdown(message["content"], unsafe_allow_html=True)
        else:
            st.markdown(message["content"])

# è·å–ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„æ³•å¾‹é—®é¢˜..."):
    # 1. å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ° session_state
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # 2. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    # 3. ç”ŸæˆåŠ©æ‰‹å›å¤
    with st.chat_message("assistant", avatar="âš–ï¸"):
        # ä½¿ç”¨å ä½ç¬¦æ˜¾ç¤º "æ€è€ƒä¸­..."
        placeholder = st.empty()
        placeholder.markdown("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ³•æ¡å¹¶ç”Ÿæˆç­”æ¡ˆ...")
        
        # è°ƒç”¨ä½ çš„ RAG æ ¸å¿ƒå‡½æ•°
        # (è¿™æ˜¯é˜»å¡æ“ä½œï¼Œä¼šç­‰å¾…æ¨¡å‹è¿”å›)
        result_dict = answer_by_rag(gen_pipe, vector_db, prompt, k=TOP_K)
        
        answer = result_dict.get("answer", "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆç­”æ¡ˆã€‚")
        sources = result_dict.get("source_documents", [])
        
        # 4. æ ¼å¼åŒ–å¹¶æ˜¾ç¤ºåŠ©æ‰‹ç­”æ¡ˆ
        full_response = f"{answer}\n\n"
        
        if sources:
            full_response += '<div class="source-container"><strong>å‚è€ƒæ¥æºï¼š</strong>\n'
            for i, doc in enumerate(sources, 1):
                # æå–å…ƒæ•°æ®å’Œå†…å®¹ç‰‡æ®µ
                source_id = doc.metadata.get('id', f'doc_{i}')
                snippet = doc.page_content.replace('\n', ' ').strip()
                snippet_preview = snippet[:150] + "..." if len(snippet) > 150 else snippet
                
                full_response += (
                    f'<details class="source-item">'
                    f'<summary>æ¥æº [{i}] (ID: {source_id})</summary>'
                    f'<div>{snippet_preview}</div>'
                    f'</details>\n'
                )
            full_response += '</div>'
        else:
            full_response += '<div class="source-container"><strong>æœªæ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ã€‚</strong></div>'

        # æ›´æ–°å ä½ç¬¦
        placeholder.markdown(full_response, unsafe_allow_html=True)
        
        # 5. å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ° session_state
        st.session_state.messages.append({"role": "assistant", "content": full_response})

#streamlit run app.py --server.address=127.0.0.1 --server.port=6006
