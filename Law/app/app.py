import os
import json
import logging
from typing import List, Tuple, Dict, Any, Optional
import jieba

import torch
import streamlit as st
import numpy as np
from rank_bm25 import BM25Okapi

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModelForSequenceClassification
from peft import PeftModel

from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from src.config import Config
from utils.set_logger import set_logger

# --- æ—¥å¿—é…ç½® ---
logger = set_logger("legal_rag_app_v2")

# ----------------- 1. æ‰€æœ‰æ¨¡å‹çš„åŠ è½½å‡½æ•° (ç¼“å­˜) -----------------

@st.cache_resource
def load_all_components(config: Config) -> Dict[str, Any]:
    """
    ä½¿ç”¨ st.cache_resource ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ¨¡å‹å’Œç´¢å¼•ã€‚åŒ…æ‹¬llm_pipeline embedding_model rerank_model index_faiss index_bm25 chunks

    Args:
        config: Config é…ç½®æ–‡ä»¶

    Returns:
        Dict[str, Any]: ç»„ä»¶å­—å…¸
    """
    
    logger.info("--- å¼€å§‹åŠ è½½æ‰€æœ‰æ¨¡å‹å’Œç´¢å¼• (ç¼“å­˜) ---")
    components = {}
    
    try:
        # 1. åŠ è½½ LLM Pipeline (æ¥è‡ª v1)
        logger.info("åŠ è½½ LLM Pipeline...")
        dtype = torch.float16 if config.device.startswith("cuda") else torch.float32
        base_model = AutoModelForCausalLM.from_pretrained(
            config.baichuan_model_path, torch_dtype=dtype, device_map="auto", trust_remote_code=True
        )
        if config.new_apapter_output_dir and os.path.exists(config.new_apapter_output_dir):
            model = PeftModel.from_pretrained(base_model, config.new_apapter_output_dir)
            logger.info(f"LoRA adapter ä» {config.new_apapter_output_dir} åŠ è½½æˆåŠŸ")
        else:
            model = base_model
        tokenizer = AutoTokenizer.from_pretrained(config.baichuan_model_path, trust_remote_code=True, use_fast=False)
        components["llm_pipe"] = pipeline(
            "text-generation", model=model, tokenizer=tokenizer, device_map="auto",
            torch_dtype=dtype, max_new_tokens=512, do_sample=True, temperature=0.7,
            top_p=0.9, repetition_penalty=1.1,
            pad_token_id=getattr(tokenizer, "eos_token_id", tokenizer.pad_token_id),
        )

        # 2. åŠ è½½ Embedding Model (æ¥è‡ª v2)
        logger.info("åŠ è½½ Embedding Model (BGE-Large)...")
        components["emb_model"] = HuggingFaceEmbeddings(
            model_name=config.bge_large_zh_model_path,
            model_kwargs={"device": config.device},
            encode_kwargs={"normalize_embeddings": True, "batch_size": 32},
        )

        # 3. åŠ è½½ Reranker Model (æ¥è‡ª v2)
        logger.info("åŠ è½½ Reranker Model (BGE-Rerank)...")
        components["rerank_tokenizer"] = AutoTokenizer.from_pretrained(config.bge_rerank_model_path)
        components["rerank_model"] = AutoModelForSequenceClassification.from_pretrained(config.bge_rerank_model_path)
        components["rerank_model"].to(config.device).eval()

        # 4. åŠ è½½ FAISS ç´¢å¼•
        logger.info("åŠ è½½ FAISS ç´¢å¼•...")
        components["vector_db"] = FAISS.load_local(
            config.faiss_index_dir, components["emb_model"], allow_dangerous_deserialization=True
        )

        # 5. åŠ è½½ BM25 ç´¢å¼• (é€šè¿‡ tokenized è¯­æ–™åº“)
        logger.info("åŠ è½½å¹¶åˆå§‹åŒ– BM25 ç´¢å¼•...")
        with open(config.tokenized_corpus_path, "r", encoding="utf-8") as f:
            tokenized_texts = json.load(f)
        components["bm25"] = BM25Okapi(tokenized_texts)

        # 6. åŠ è½½ Chunks è¯­æ–™åº“
        logger.info("åŠ è½½ Chunks è¯­æ–™åº“...")
        chunks = []
        with open(config.chunks_corpus_path, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)
                chunks.append(Document(page_content=data["page_content"], metadata=data["metadata"]))
        components["chunks"] = chunks
        
        logger.info(f"--- æ‰€æœ‰ {len(components)} ä¸ªç»„ä»¶åŠ è½½å®Œæ¯• ---")
        return components

    except Exception as e:
        logger.error(f"åŠ è½½ç»„ä»¶æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        st.error(f"æ¨¡å‹/ç´¢å¼•åŠ è½½å¤±è´¥: {e}\nè¯·æ£€æŸ¥è·¯å¾„: {config.base_model_path}, {config.bge_large_zh_model_path}, {config.bge_rerank_model_path}, {config.faiss_index_dir}, {config.tokenized_corpus_path}, {config.chunks_corpus_path}")
        st.stop()


# ----------------- 2. RAG é€»è¾‘å‡½æ•° -----------------

# --- 2a. å…±äº«çš„ Prompt æ¨¡æ¿ ---
def compose_prompt_with_context(question: str, docs: List[Document]) -> str:
    """
    (å…±äº«) ç®€å• prompt æ‹¼æ¥ç­–ç•¥ã€‚
    """
    ctxs = []
    for i, d in enumerate(docs, 1):
        snippet = d.page_content.strip()
        ctxs.append(f"[{i}] {snippet}")
    context_block = "\n\n".join(ctxs)
    
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå…·æœ‰æ³•å¾‹ä¸“ä¸šçŸ¥è¯†çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä»…åŸºäºä¸‹é¢æä¾›çš„ä¸Šä¸‹æ–‡(Context)å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œ"
        "å¹¶åœ¨ç­”æ¡ˆæœ«å°¾åˆ—å‡ºä½ å¼•ç”¨çš„æ–‡æ¡£ç¼–å·ã€‚\n\n"
        f"Context:\n{context_block}\n\nQuestion: {question}\n\nAnswer:"
    )
    return prompt

# --- 2b. æ ‡å‡† RAG é€»è¾‘ (v1) ---
def answer_by_rag_simple(
    llm_pipe, 
    vector_db: FAISS, 
    question: str, 
    k: int
) -> Tuple[str, List[Document]]:
    """
    æ‰§è¡Œæ ‡å‡† RAG:
    1. FAISS æ£€ç´¢
    2. æ„å»º Prompt
    3. LLM ç”Ÿæˆ
    """
    logger.info("æ‰§è¡Œ [æ ‡å‡† RAG] æµç¨‹...")
    # 1. æ£€ç´¢
    retrieved = vector_db.similarity_search(question, k=k)
    
    # 2. æ„å»º prompt
    prompt = compose_prompt_with_context(question, retrieved)

    # 3. ç”Ÿæˆ
    out = llm_pipe(prompt)
    raw_text = out[0].get("generated_text", "")
    if raw_text.startswith(prompt):
        answer = raw_text[len(prompt):].strip()
    else:
        answer = raw_text.strip()

    return answer, retrieved

# --- 2c. é«˜çº§ RAG é€»è¾‘ (v2) ---
from utils.retrieve_bm25_topk import retrieve_bm25_topk
from utils.retrieve_emb_topk import retrieve_emb_topk
from utils.rrf_fusion import rrf_fusion
from utils.multi_retrieve_and_rerank import multi_retrieve_and_rerank

def answer_by_rag_advanced(
    llm_pipe, 
    vector_db: FAISS, 
    bm25: BM25Okapi,
    chunks: List[Document],
    rerank_tokenizer: AutoTokenizer,
    rerank_model: AutoModelForSequenceClassification,
    config: Config,
    question: str
) -> Tuple[str, List[Document]]:
    """
    æ‰§è¡Œé«˜çº§ RAG:
    1. v2 æ£€ç´¢ (RRF + Rerank)
    2. æ„å»º Prompt
    3. LLM ç”Ÿæˆ
    """
    # 1. v2 æ£€ç´¢
    retrieved_docs = multi_retrieve_and_rerank(
        question, vector_db, bm25, chunks, rerank_tokenizer, rerank_model, config
    )
    
    # 2. æ„å»º prompt
    prompt = compose_prompt_with_context(question, retrieved_docs)

    # 3. ç”Ÿæˆ
    out = llm_pipe(prompt)
    raw_text = out[0].get("generated_text", "")
    if raw_text.startswith(prompt):
        answer = raw_text[len(prompt):].strip()
    else:
        answer = raw_text.strip()

    return answer, retrieved_docs

# ----------------- 3. Streamlit UI ç•Œé¢ -----------------

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="æ³•å¾‹é—®ç­”å¤§æ¨¡å‹",
    page_icon="âš–ï¸",
    initial_sidebar_state="auto"
)

# --- CSS æ ·å¼ (æ¥è‡ªä½ çš„ minimind ç¤ºä¾‹) ---
st.markdown("""
    <style>
        /* (è¿™é‡Œçœç•¥äº†ä½ æä¾›çš„é•¿ä¸² CSSï¼Œä¿æŒåŸæ ·) */
        /* ... ä½ æä¾›çš„æ‰€æœ‰ .stButton, .stMainBlockContainer, .stApp æ ·å¼ ... */
        
        /* æ¥æºæ–‡æ¡£çš„æ ·å¼ */
        .source-container {
            border-top: 1px solid #eee;
            margin-top: 15px;
            padding-top: 10px;
        }
        .source-item {
            font-size: 0.9em;
            color: #555;
            background-color: #f9f9f9;
            border-radius: 5px;
            padding: 8px;
            margin-bottom: 5px;
            border: 1px solid #eee;
        }
        .source-item summary {
            font-weight: bold;
            cursor: pointer;
        }
    </style>
""", unsafe_allow_html=True)

# --- æ ‡é¢˜ ---
st.markdown(
    f'<div style="display: flex; flex-direction: column; align-items: center; text-align: center; margin: 0; padding: 0;">'
    f'<span style="font-size: 26px; font-weight: 900; margin-left: 10px;">âš–ï¸ æ³•å¾‹ RAG é—®ç­” (åŒæ¨¡å¼)</span>'
    '</div>',
    unsafe_allow_html=True
)

# --- ä¾§è¾¹æ  ---
st.sidebar.title("ğŸ› ï¸ RAG æ¨¡å¼è®¾ç½®")
st.sidebar.toggle(
    "ğŸ”¬ æ·±åº¦æ£€ç´¢ (é«˜çº§RAG)", 
    value=False, 
    key="deep_rag_toggle",
    help="å¼€å¯åï¼Œå°†ä½¿ç”¨ BM25+Embedding+RRF+Rerank çš„é«˜çº§æ£€ç´¢æ¨¡å¼ï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†å¯èƒ½æ›´å‡†ã€‚å…³é—­åˆ™ä½¿ç”¨å¿«é€Ÿçš„ FAISS æ£€ç´¢ã€‚"
)
st.sidebar.markdown("---")
if st.sidebar.button("æ¸…ç©ºèŠå¤©è®°å½•", use_container_width=True):
    st.session_state.messages = []
    st.rerun()

# --- åŠ è½½æ‰€æœ‰ç»„ä»¶ ---
config = Config()
try:
    components = load_all_components(config)
    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºåŠ è½½æˆåŠŸçŠ¶æ€
    st.sidebar.success("æ‰€æœ‰æ¨¡å‹å’Œç´¢å¼•åŠ è½½æˆåŠŸï¼")
    st.sidebar.markdown(f"**LLM**: {config.base_model_path}\n"
                        f"**Reranker**: {config.rerank_model_path}\n"
                        f"**Chunks**: {len(components['chunks'])} æ¡", 
                        unsafe_allow_html=True)
except Exception:
    # é”™è¯¯å·²åœ¨åŠ è½½å‡½æ•°ä¸­é€šè¿‡ st.error å’Œ st.stop å¤„ç†
    pass


# --- èŠå¤©ç•Œé¢ ---

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    avatar = "âš–ï¸" if message["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"], unsafe_allow_html=True)

# è·å–ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„æ³•å¾‹é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    # æ ¹æ® Toggle çŠ¶æ€å†³å®šè°ƒç”¨å“ªä¸ª RAG æµç¨‹
    is_deep_rag = st.session_state.deep_rag_toggle
    
    with st.chat_message("assistant", avatar="âš–ï¸"):
        placeholder = st.empty()
        
        try:
            if is_deep_rag:
                # --- é«˜çº§ RAG æµç¨‹ (v2) ---
                placeholder.markdown("ğŸ”¬ **æ·±åº¦æ£€ç´¢ä¸­...** (æ‰§è¡Œ é«˜çº§RAG)")
                answer, sources = answer_by_rag_advanced(
                    llm_pipe=components["llm_pipe"],
                    vector_db=components["vector_db"],
                    bm25=components["bm25"],
                    chunks=components["chunks"],
                    rerank_tokenizer=components["rerank_tokenizer"],
                    rerank_model=components["rerank_model"],
                    config=config,
                    question=prompt
                )
            else:
                # --- æ ‡å‡† RAG æµç¨‹ (v1) ---
                placeholder.markdown("ğŸ” **æ ‡å‡†æ£€ç´¢ä¸­...** (æ‰§è¡Œ åˆçº§RAG)")
                answer, sources = answer_by_rag_simple(
                    llm_pipe=components["llm_pipe"],
                    vector_db=components["vector_db"],
                    question=prompt,
                    k=config.topk_simple
                )

            # æ ¼å¼åŒ–å¹¶æ˜¾ç¤ºåŠ©æ‰‹ç­”æ¡ˆ + æ¥æº
            full_response = f"{answer}\n\n"
            if sources:
                full_response += '<div class="source-container"><strong>å‚è€ƒæ¥æºï¼š</strong>\n'
                for i, doc in enumerate(sources, 1):
                    source_id = doc.metadata.get('id', f'doc_{i}')
                    snippet = doc.page_content.replace('\n', ' ').strip()
                    snippet_preview = snippet[:150] + "..." if len(snippet) > 150 else snippet
                    
                    full_response += (
                        f'<details class="source-item">'
                        f'<summary>æ¥æº [{i}] (ID: {source_id})</summary>'
                        f'<div>{snippet_preview}</div>'
                        f'</details>\n'
                    )
                full_response += '</div>'
            else:
                full_response += '<div class="source-container"><strong>æœªèƒ½æ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ã€‚</strong></div>'

            # æ›´æ–°å ä½ç¬¦
            placeholder.markdown(full_response, unsafe_allow_html=True)
            # å­˜å…¥å†å²è®°å½•
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            logger.error(f"åœ¨å¤„ç†æŸ¥è¯¢ '{prompt}' æ—¶å‡ºé”™: {e}", exc_info=True)
            placeholder.error(f"å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {e}")
            st.session_state.messages.append({"role": "assistant", "content": f"é”™è¯¯: {e}"})

# streamlit run app_with_toggle.py --server.address=127.0.0.1 --server.port=6006
